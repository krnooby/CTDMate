"""
ìš©ì–´ì§‘ ì²­í‚¹ ìœ í‹¸ë¦¬í‹°

JSONL í˜•ì‹ì˜ ìš©ì–´ì§‘ íŒŒì¼ì„ ë‹¤ì–‘í•œ í˜•íƒœë¡œ ì²­í‚¹í•©ë‹ˆë‹¤.
- JSON ì›ë³¸ ìœ ì§€
- êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ë³€í™˜
- ë”•ì…”ë„ˆë¦¬ ê°ì²´ ë°˜í™˜
"""

import json
from typing import List, Dict, Any


def chunk_glossary_as_json_string(file_path: str) -> List[str]:
    """
    ìš©ì–´ì§‘ JSONL íŒŒì¼ì„ ì½ì–´ ê° JSON ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜

    :param file_path: ì²˜ë¦¬í•  JSONL íŒŒì¼ ê²½ë¡œ
    :return: JSON ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ (ê° í•­ëª©ì´ í•˜ë‚˜ì˜ ì²­í¬)

    Example:
        >>> chunks = chunk_glossary_as_json_string('glossary.jsonl')
        >>> chunks[0]
        '{"category": "R&D", "term": "ADME", ...}'
    """
    print(f"ğŸ“– '{file_path}' íŒŒì¼ì—ì„œ ìš©ì–´ì§‘ ë°ì´í„°ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤ (JSON ì›ë³¸ ìœ ì§€)...")
    json_chunks = []

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if not line.strip():
                    continue

                try:
                    # JSON ìœ íš¨ì„± ê²€ì‚¬
                    json.loads(line)
                    # ì›ë³¸ JSON ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ì €ì¥
                    json_chunks.append(line.strip())

                except json.JSONDecodeError as e:
                    print(f"âš ï¸  JSON íŒŒì‹± ì˜¤ë¥˜ (ë¼ì¸ {line_num}): {e}")
                    continue

    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: '{file_path}'")
        return []

    print(f"âœ… ì´ {len(json_chunks)}ê°œì˜ ìš©ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n")
    return json_chunks


def chunk_glossary_as_dict(file_path: str) -> List[Dict[str, Any]]:
    """
    ìš©ì–´ì§‘ JSONL íŒŒì¼ì„ ì½ì–´ ë”•ì…”ë„ˆë¦¬ ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜

    :param file_path: ì²˜ë¦¬í•  JSONL íŒŒì¼ ê²½ë¡œ
    :return: ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸

    Example:
        >>> chunks = chunk_glossary_as_dict('glossary.jsonl')
        >>> chunks[0]['term']
        'ADME'
    """
    print(f"ğŸ“– '{file_path}' íŒŒì¼ì—ì„œ ìš©ì–´ì§‘ ë°ì´í„°ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤ (ë”•ì…”ë„ˆë¦¬)...")
    dict_chunks = []

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if not line.strip():
                    continue

                try:
                    item = json.loads(line)
                    dict_chunks.append(item)

                except json.JSONDecodeError as e:
                    print(f"âš ï¸  JSON íŒŒì‹± ì˜¤ë¥˜ (ë¼ì¸ {line_num}): {e}")
                    continue

    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: '{file_path}'")
        return []

    print(f"âœ… ì´ {len(dict_chunks)}ê°œì˜ ìš©ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n")
    return dict_chunks


def chunk_glossary_as_text(file_path: str) -> List[str]:
    """
    ìš©ì–´ì§‘ JSONL íŒŒì¼ì„ ì½ì–´ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    (ê¸°ì¡´ ë°©ì‹ - í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)

    :param file_path: ì²˜ë¦¬í•  JSONL íŒŒì¼ ê²½ë¡œ
    :return: í…ìŠ¤íŠ¸ ì²­í¬(ë¬¸ì„œ) ë¦¬ìŠ¤íŠ¸

    Example:
        >>> chunks = chunk_glossary_as_text('glossary.jsonl')
        >>> print(chunks[0])
        ì¹´í…Œê³ ë¦¬: R&D
        ìš©ì–´: ADME (Absorption, Distribution, Metabolism, Excretion)
        ì„¤ëª…: í¡ìˆ˜Â·ë¶„í¬Â·ëŒ€ì‚¬Â·ë°°ì„¤...
        ë™ì˜ì–´: ì•½ë™í•™ê°œìš”
    """
    print(f"ğŸ“– '{file_path}' íŒŒì¼ì—ì„œ ìš©ì–´ì§‘ ë°ì´í„°ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤ (êµ¬ì¡°í™” í…ìŠ¤íŠ¸)...")
    text_chunks = []

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if not line.strip():
                    continue

                try:
                    item = json.loads(line)
                    description = item.get('description_ko') or item.get('definition_ko', 'ì„¤ëª… ì •ë³´ ì—†ìŒ')
                    synonyms_list = item.get('synonyms') or []
                    synonyms_str = ', '.join(synonyms_list) if synonyms_list else 'ì—†ìŒ'

                    chunk_text = (
                        f"ì¹´í…Œê³ ë¦¬: {item.get('category', 'ì •ë³´ ì—†ìŒ')}\n"
                        f"ìš©ì–´: {item.get('term', 'ì •ë³´ ì—†ìŒ')} ({item.get('term_en', 'ì •ë³´ ì—†ìŒ')})\n"
                        f"ì„¤ëª…: {description}\n"
                        f"ë™ì˜ì–´: {synonyms_str}"
                    )
                    text_chunks.append(chunk_text)

                except json.JSONDecodeError as e:
                    print(f"âš ï¸  JSON íŒŒì‹± ì˜¤ë¥˜ (ë¼ì¸ {line_num}): {e}")
                    continue

    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: '{file_path}'")
        return []

    print(f"âœ… ì´ {len(text_chunks)}ê°œì˜ ìš©ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n")
    return text_chunks


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
chunk_glossary_jsonl = chunk_glossary_as_text


if __name__ == "__main__":
    """ëª¨ë“ˆ í…ŒìŠ¤íŠ¸"""
    test_file = 'data/glossary/glossary_final.jsonl'

    print("=" * 70)
    print("  Chunking Utils í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    print()

    # í…ŒìŠ¤íŠ¸ 1: JSON ë¬¸ìì—´
    print("í…ŒìŠ¤íŠ¸ 1: JSON ë¬¸ìì—´ ì²­í‚¹")
    print("-" * 70)
    json_chunks = chunk_glossary_as_json_string(test_file)
    if json_chunks:
        print(f"ì²« ë²ˆì§¸ ì²­í¬ (JSON):\n{json_chunks[0]}\n")

    # í…ŒìŠ¤íŠ¸ 2: ë”•ì…”ë„ˆë¦¬
    print("\ní…ŒìŠ¤íŠ¸ 2: ë”•ì…”ë„ˆë¦¬ ì²­í‚¹")
    print("-" * 70)
    dict_chunks = chunk_glossary_as_dict(test_file)
    if dict_chunks:
        print(f"ì²« ë²ˆì§¸ ì²­í¬ (Dict):\n{dict_chunks[0]}\n")

    # í…ŒìŠ¤íŠ¸ 3: í…ìŠ¤íŠ¸ (ê¸°ì¡´ ë°©ì‹)
    print("\ní…ŒìŠ¤íŠ¸ 3: í…ìŠ¤íŠ¸ ì²­í‚¹ (ê¸°ì¡´ ë°©ì‹)")
    print("-" * 70)
    text_chunks = chunk_glossary_as_text(test_file)
    if text_chunks:
        print(f"ì²« ë²ˆì§¸ ì²­í¬ (Text):\n{text_chunks[0]}\n")

    print("=" * 70)
    print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("=" * 70)