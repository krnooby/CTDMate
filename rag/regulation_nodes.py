"""
RegulationTool Nodes

LangGraph StateGraphì˜ ë…¸ë“œ í•¨ìˆ˜ë“¤
- validate_node: ê·œì • ê²€ì¦ (ì²´í¬ë¦¬ìŠ¤íŠ¸ ë§¤ì¹­)
- normalize_node: ìš©ì–´ ì •ê·œí™” (Llama-3.2-1B)
- expand_coverage_node: ìŠ¤ë‹ˆí« í™•ì¥ (BM25 + Vector + MMR)
- collect_citations_node: ì¸ìš© ìˆ˜ì§‘
"""

import sys
from pathlib import Path
from typing import Dict, List, Any
import re

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from tools.regulation_state import RegulationState, Violation
from tools.mfds_rag import MFDSRAGTool
from tools.glossary_rag import GlossaryRAGTool
from tools.bm25_search import BM25Search


def validate_node(state: RegulationState) -> RegulationState:
    """
    ê·œì • ê²€ì¦ ë…¸ë“œ

    MFDS RAGë¡œ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰í•˜ê³ , composition_dataì™€ ë¹„êµí•˜ì—¬ ìœ„ë°˜ ì‚¬í•­ íƒì§€

    Args:
        state: í˜„ì¬ ìƒíƒœ

    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (violations, violation_severity, pass_validation)
    """
    print(f"\nğŸ” [VALIDATE] CTD ì„¹ì…˜ {state['ctd_section']} ê²€ì¦ ì‹œì‘...")

    # 1. MFDS RAGë¡œ ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²€ìƒ‰
    rag = MFDSRAGTool()
    section_info = rag.get_section_by_id(state["ctd_section"])

    if not section_info:
        print(f"âš ï¸  ì„¹ì…˜ {state['ctd_section']}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²€ì¦ ìŠ¤í‚µ.")
        return {
            **state,
            "validated": True,
            "pass_validation": True,
            "violations": [],
            "violation_severity": "none"
        }

    checklist = section_info.get("checklist", [])
    print(f"ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª© ìˆ˜: {len(checklist)}")

    # 2. ê° ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª© ê²€ì¦
    violations: List[Violation] = []
    composition = state["composition_data"]

    for item in checklist:
        violation = _check_compliance(composition, item, state["ctd_section"])
        if violation:
            violations.append(violation)

    # 3. ìœ„ë°˜ ì‹¬ê°ë„ íŒë‹¨
    severity = _determine_severity(violations)
    print(f"ğŸ“Š ê²€ì¦ ê²°ê³¼: {len(violations)}ê°œ ìœ„ë°˜ (ì‹¬ê°ë„: {severity})")

    # 4. ìƒíƒœ ì—…ë°ì´íŠ¸
    return {
        **state,
        "validated": True,
        "violations": violations,
        "violation_severity": severity,
        "pass_validation": len(violations) == 0,
        "need_normalize": severity == "major"
    }


def normalize_node(state: RegulationState) -> RegulationState:
    """
    ìš©ì–´ ì •ê·œí™” ë…¸ë“œ

    Major ìœ„ë°˜ ì‚¬í•­ì˜ í•„ë“œë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”
    í˜„ì¬ëŠ” glossary RAG ê¸°ë°˜, ì¶”í›„ Llama-3.2-1B LoRAë¡œ êµì²´ ì˜ˆì •

    Args:
        state: í˜„ì¬ ìƒíƒœ

    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (normalized_fields, normalization_applied)
    """
    print(f"\nğŸ”§ [NORMALIZE] ìš©ì–´ ì •ê·œí™” ì‹œì‘ (ë°˜ë³µ {state['current_iteration'] + 1}/{state['max_iterations']})...")

    normalized = {}
    glossary_rag = GlossaryRAGTool()

    for violation in state["violations"]:
        if violation["severity"] == "major":
            field = violation["field"]
            current_value = violation["current_value"]

            # Glossary RAGë¡œ í‘œì¤€ ìš©ì–´ ê²€ìƒ‰
            results = glossary_rag.search_term(str(current_value), k=1, score_threshold=0.7)

            if results:
                # ê°€ì¥ ìœ ì‚¬í•œ ìš©ì–´ì˜ ì˜ë¬¸ëª… ì‚¬ìš©
                term_data = glossary_rag.parse_json_content(results[0]['content'])
                normalized[field] = term_data.get('term_en', current_value)
                print(f"  âœ“ {field}: '{current_value}' â†’ '{normalized[field]}'")
            else:
                print(f"  âš ï¸  {field}: '{current_value}' - í‘œì¤€ ìš©ì–´ ë¯¸ë°œê²¬")

    print(f"ğŸ“Š ì •ê·œí™” ì™„ë£Œ: {len(normalized)}ê°œ í•„ë“œ")

    return {
        **state,
        "normalized_fields": {**state.get("normalized_fields", {}), **normalized},
        "normalization_applied": len(normalized) > 0,
        "current_iteration": state["current_iteration"] + 1
    }


def expand_coverage_node(state: RegulationState) -> RegulationState:
    """
    ì»¤ë²„ë¦¬ì§€ í™•ì¥ ë…¸ë“œ

    BM25 + Vector + MMR í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ìŠ¤ë‹ˆí« í™•ì¥

    Args:
        state: í˜„ì¬ ìƒíƒœ

    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (retrieved_snippets, coverage_score)
    """
    print(f"\nğŸ“š [EXPAND] ì»¤ë²„ë¦¬ì§€ í™•ì¥ ì¤‘ (BM25 + Vector + MMR)...")

    query = f"{state['ctd_section']} ì‘ì„± ìš”êµ¬ì‚¬í•­"

    # 1. BM25 í‚¤ì›Œë“œ ê²€ìƒ‰
    bm25 = BM25Search()
    bm25_results = bm25.search(query, k=10)
    print(f"  âœ“ BM25 ê²€ìƒ‰: {len(bm25_results)}ê°œ")

    # 2. Vector ê²€ìƒ‰ (MFDS ê°€ì´ë“œë¼ì¸)
    rag = MFDSRAGTool()
    vector_results = rag.search_guideline(query, k=10)
    print(f"  âœ“ Vector ê²€ìƒ‰: {len(vector_results)}ê°œ")

    # 3. ê²°ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°, ì ìˆ˜ ê¸°ë°˜)
    combined = _merge_search_results(bm25_results, vector_results)
    print(f"  âœ“ ë³‘í•© ê²°ê³¼: {len(combined)}ê°œ")

    # 4. MMR ë‹¤ì–‘ì„± í•„í„°ë§
    mmr_results = rag.search_with_mmr(
        query=query,
        k=5,
        fetch_k=20,
        lambda_mult=0.5  # ê´€ë ¨ì„± 50%, ë‹¤ì–‘ì„± 50%
    )
    print(f"  âœ“ MMR í•„í„°ë§: {len(mmr_results)}ê°œ")

    # 5. ê¸°ì¡´ ìŠ¤ë‹ˆí«ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°)
    existing_ids = {s.get("section") for s in state.get("retrieved_snippets", [])}
    new_snippets = [s for s in mmr_results if s.get("section") not in existing_ids]

    # BM25 ê²°ê³¼ ì¤‘ ìƒìœ„ ì ìˆ˜ë„ ì¶”ê°€
    for r in combined[:3]:
        if r.get("section") not in existing_ids:
            new_snippets.append(r)
            existing_ids.add(r.get("section"))

    all_snippets = state.get("retrieved_snippets", []) + new_snippets

    # 6. ì»¤ë²„ë¦¬ì§€ ì ìˆ˜ ê³„ì‚°
    coverage = _calculate_coverage(state["composition_data"], all_snippets)

    print(f"ğŸ“Š ìŠ¤ë‹ˆí« ì¶”ê°€: {len(new_snippets)}ê°œ (ì´ {len(all_snippets)}ê°œ)")
    print(f"ğŸ“ˆ ì»¤ë²„ë¦¬ì§€: {coverage:.2%}")

    return {
        **state,
        "retrieved_snippets": all_snippets,
        "coverage_score": coverage,
        "need_expand_coverage": coverage < 0.8 and state["current_iteration"] < state["max_iterations"]
    }


def collect_citations_node(state: RegulationState) -> RegulationState:
    """
    ì¸ìš© ìˆ˜ì§‘ ë…¸ë“œ

    ê²€ìƒ‰ëœ ìŠ¤ë‹ˆí«ì—ì„œ ì¸ìš© ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

    Args:
        state: í˜„ì¬ ìƒíƒœ

    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (citations)
    """
    print(f"\nğŸ“– [CITATIONS] ì¸ìš© ì •ë³´ ìˆ˜ì§‘ ì¤‘...")

    citations = []
    for snippet in state.get("retrieved_snippets", []):
        citations.append({
            "source": f"MFDS {snippet.get('module', 'N/A')}",
            "module": snippet.get("module", "N/A"),
            "section": snippet.get("section", "N/A"),
            "title": snippet.get("title", "N/A"),
            "snippet_id": snippet.get("metadata", {}).get("para_id", "N/A"),
            "page": None  # MFDS ë°ì´í„°ì—ëŠ” í˜ì´ì§€ ì •ë³´ ì—†ìŒ
        })

    print(f"ğŸ“Š ì¸ìš© ìˆ˜ì§‘ ì™„ë£Œ: {len(citations)}ê°œ")

    return {
        **state,
        "citations": citations
    }


# ========== Helper Functions ==========

def _check_compliance(composition: Dict[str, Any], rule: str, section: str) -> Violation | None:
    """
    ë‹¨ì¼ ì²´í¬ë¦¬ìŠ¤íŠ¸ ê·œì¹™ ê²€ì¦

    Args:
        composition: ì¡°ì„± ë°ì´í„°
        rule: ì²´í¬ë¦¬ìŠ¤íŠ¸ ê·œì¹™ ë¬¸ìì—´
        section: CTD ì„¹ì…˜

    Returns:
        ìœ„ë°˜ ì‚¬í•­ ë˜ëŠ” None
    """
    # ê·œì¹™ì—ì„œ í•„ìˆ˜ í•„ë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
    required_fields = _extract_required_fields(rule)

    for field in required_fields:
        # compositionì— í•„ë“œê°€ ì—†ê±°ë‚˜ ë¹ˆ ê°’ì´ë©´ ìœ„ë°˜
        value = composition.get(field)
        if not value or (isinstance(value, str) and not value.strip()):
            return Violation(
                rule=rule,
                severity="major" if "must" in rule.lower() or "í•„ìˆ˜" in rule else "minor",
                field=field,
                current_value=value,
                expected_format=rule
            )

    return None


def _extract_required_fields(rule: str) -> List[str]:
    """
    ì²´í¬ë¦¬ìŠ¤íŠ¸ ê·œì¹™ì—ì„œ í•„ìˆ˜ í•„ë“œëª… ì¶”ì¶œ

    ì˜ˆ: "ì„±ë¶„ëª… ë° í•¨ëŸ‰ ëª…ì‹œ" â†’ ["dl_material", "dl_material_en"]

    Args:
        rule: ì²´í¬ë¦¬ìŠ¤íŠ¸ ê·œì¹™ ë¬¸ìì—´

    Returns:
        í•„ë“œëª… ë¦¬ìŠ¤íŠ¸
    """
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤í•‘ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë§¤í•‘ í•„ìš”)
    keyword_mapping = {
        "ì„±ë¶„": ["dl_material", "dl_material_en"],
        "ì œí˜•": ["dl_custom_shape"],
        "ì œì¡°": ["dl_company", "dl_company_en"],
        "ìš©ëŸ‰": ["leng_long", "leng_short", "thick"],
        "ìƒ‰ìƒ": ["color_class1", "color_class2"],
    }

    fields = []
    for keyword, field_names in keyword_mapping.items():
        if keyword in rule:
            fields.extend(field_names)

    return fields if fields else ["dl_material"]  # ê¸°ë³¸ê°’


def _determine_severity(violations: List[Violation]) -> str:
    """
    ìœ„ë°˜ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì „ì²´ ì‹¬ê°ë„ íŒë‹¨

    Args:
        violations: ìœ„ë°˜ ì‚¬í•­ ë¦¬ìŠ¤íŠ¸

    Returns:
        "major" | "minor" | "none"
    """
    if not violations:
        return "none"

    has_major = any(v["severity"] == "major" for v in violations)
    return "major" if has_major else "minor"


def _merge_search_results(
    bm25_results: List[Dict[str, Any]],
    vector_results: List[Dict[str, Any]],
    bm25_weight: float = 0.4,
    vector_weight: float = 0.6
) -> List[Dict[str, Any]]:
    """
    BM25ì™€ Vector ê²€ìƒ‰ ê²°ê³¼ ë³‘í•© (ê°€ì¤‘ ì ìˆ˜ ê¸°ë°˜)

    Args:
        bm25_results: BM25 ê²€ìƒ‰ ê²°ê³¼
        vector_results: Vector ê²€ìƒ‰ ê²°ê³¼
        bm25_weight: BM25 ì ìˆ˜ ê°€ì¤‘ì¹˜ (ê¸°ë³¸: 0.4)
        vector_weight: Vector ì ìˆ˜ ê°€ì¤‘ì¹˜ (ê¸°ë³¸: 0.6)

    Returns:
        ë³‘í•©ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ)
    """
    # ì„¹ì…˜ë³„ë¡œ ì ìˆ˜ ì§‘ê³„
    score_map = {}

    # BM25 ì ìˆ˜ ì¶”ê°€
    for r in bm25_results:
        section = r.get("section")
        if section:
            score_map[section] = {
                "data": r,
                "bm25_score": r.get("score", 0.0) * bm25_weight,
                "vector_score": 0.0
            }

    # Vector ì ìˆ˜ ì¶”ê°€ (ê¸°ì¡´ ì„¹ì…˜ì´ë©´ vector_scoreë§Œ ì—…ë°ì´íŠ¸)
    for r in vector_results:
        section = r.get("section")
        if section:
            if section in score_map:
                score_map[section]["vector_score"] = r.get("score", 0.0) * vector_weight
            else:
                score_map[section] = {
                    "data": r,
                    "bm25_score": 0.0,
                    "vector_score": r.get("score", 0.0) * vector_weight
                }

    # í•©ì‚° ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
    merged = []
    for section, info in score_map.items():
        combined_score = info["bm25_score"] + info["vector_score"]
        result = info["data"].copy()
        result["combined_score"] = combined_score
        merged.append(result)

    # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    merged.sort(key=lambda x: x.get("combined_score", 0.0), reverse=True)

    return merged


def _calculate_coverage(composition: Dict[str, Any], snippets: List[Dict[str, Any]]) -> float:
    """
    ìŠ¤ë‹ˆí«ì´ composition ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì»¤ë²„í•˜ëŠ”ì§€ ê³„ì‚°

    ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ìŠ¤ë‹ˆí« ê°œìˆ˜ ê¸°ë°˜ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë¡œì§ í•„ìš”)

    Args:
        composition: ì¡°ì„± ë°ì´í„°
        snippets: ê²€ìƒ‰ëœ ìŠ¤ë‹ˆí« ë¦¬ìŠ¤íŠ¸

    Returns:
        ì»¤ë²„ë¦¬ì§€ ì ìˆ˜ (0~1)
    """
    if not snippets:
        return 0.0

    # ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹±: ìŠ¤ë‹ˆí« 5ê°œ ì´ìƒì´ë©´ 80% ì»¤ë²„
    return min(len(snippets) / 5.0, 1.0) * 0.8
